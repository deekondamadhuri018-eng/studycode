<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>codestude</title>
</head>
<body>
    import streamlit as st
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
import torch
from typing import List, Tuple
import tempfile
import os

# Page Configuration
st.set_page_config(
    page_title="StudyMate - AI Academic Assistant",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .answer-box {
        background-color: #f0f7ff;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #1E88E5;
        margin: 10px 0;
    }
    .context-box {
        background-color: #f9f9f9;
        padding: 15px;
        border-radius: 8px;
        border-left: 3px solid #FFA726;
        margin: 10px 0;
        font-size: 0.9rem;
    }
    .stButton>button {
        width: 100%;
        background-color: #1E88E5;
        color: white;
        font-weight: bold;
        border-radius: 8px;
        padding: 0.5rem 1rem;
    }
    </style>
""", unsafe_allow_html=True)

class StudyMate:
    def __init__(self):
        self.embedding_model = None
        self.llm_model = None
        self.tokenizer = None
        self.index = None
        self.chunks = []
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    @st.cache_resource
    def load_embedding_model(_self):
        """Load SentenceTransformer for embeddings"""
        return SentenceTransformer('all-MiniLM-L6-v2')
    
    @st.cache_resource
    def load_llm_model(_self):
        """Load IBM Granite model for answer generation"""
        model_path = "ibm-granite/granite-3.3-2b-instruct"
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map=_self.device,
            torch_dtype=torch.bfloat16,
        )
        
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        return model, tokenizer
    
    def extract_text_from_pdf(self, pdf_file) -> str:
        """Extract text from uploaded PDF using PyMuPDF"""
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(pdf_file.read())
            tmp_path = tmp_file.name
        
        try:
            doc = fitz.open(tmp_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        finally:
            os.unlink(tmp_path)
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk:
                chunks.append(chunk)
        
        return chunks
    
    def build_faiss_index(self, chunks: List[str]):
        """Build FAISS index from text chunks"""
        if self.embedding_model is None:
            self.embedding_model = self.load_embedding_model()
        
        embeddings = self.embedding_model.encode(chunks, show_progress_bar=True)
        embeddings = np.array(embeddings).astype('float32')
        
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings)
        self.chunks = chunks
    
    def retrieve_relevant_chunks(self, question: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """Retrieve most relevant chunks for a question"""
        if self.index is None:
            return []
        
        question_embedding = self.embedding_model.encode([question])
        question_embedding = np.array(question_embedding).astype('float32')
        
        distances, indices = self.index.search(question_embedding, top_k)
        
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.chunks):
                results.append((self.chunks[idx], float(dist)))
        
        return results
    
    def generate_answer(self, question: str, context: str) -> str:
        """Generate answer using IBM Granite model"""
        if self.llm_model is None or self.tokenizer is None:
            self.llm_model, self.tokenizer = self.load_llm_model()
        
        prompt = f"""Based on the following context from academic materials, answer the question accurately and concisely.

Context: {context}

Question: {question}

Answer the question using only the information from the context. If the context doesn't contain enough information, say so."""
        
        conv = [{"role": "user", "content": prompt}]
        
        input_ids = self.tokenizer.apply_chat_template(
            conv, 
            return_tensors="pt", 
            thinking=True, 
            return_dict=True, 
            add_generation_prompt=True
        ).to(self.device)
        
        set_seed(42)
        output = self.llm_model.generate(
            **input_ids,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )
        
        answer = self.tokenizer.decode(
            output[0, input_ids["input_ids"].shape[1]:], 
            skip_special_tokens=True
        )
        
        return answer.strip()

def main():
    # Header
    st.markdown('<p class="main-header">üìö StudyMate</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Your AI-Powered Academic Assistant</p>', unsafe_allow_html=True)
    
    # Initialize StudyMate
    if 'studymate' not in st.session_state:
        st.session_state.studymate = StudyMate()
    
    if 'processed' not in st.session_state:
        st.session_state.processed = False
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    studymate = st.session_state.studymate
    
    # Sidebar for PDF Upload
    with st.sidebar:
        st.header("üìÑ Upload Study Materials")
        st.write("Upload your PDFs to get started")
        
        uploaded_files = st.file_uploader(
            "Choose PDF files",
            type=['pdf'],
            accept_multiple_files=True,
            help="Upload one or more PDF documents"
        )
        
        if uploaded_files and st.button("üîÑ Process Documents"):
            with st.spinner("Processing your documents..."):
                all_text = ""
                progress_bar = st.progress(0)
                
                for idx, pdf_file in enumerate(uploaded_files):
                    st.info(f"Processing: {pdf_file.name}")
                    text = studymate.extract_text_from_pdf(pdf_file)
                    all_text += text + "\n\n"
                    progress_bar.progress((idx + 1) / len(uploaded_files))
                
                # Chunk and index
                st.info("Creating searchable index...")
                chunks = studymate.chunk_text(all_text)
                studymate.build_faiss_index(chunks)
                
                st.session_state.processed = True
                st.success(f"‚úÖ Processed {len(uploaded_files)} document(s) with {len(chunks)} chunks!")
        
        if st.session_state.processed:
            st.success("‚úì Documents ready for questions!")
            
            if st.button("üóëÔ∏è Clear All"):
                st.session_state.processed = False
                st.session_state.chat_history = []
                st.session_state.studymate = StudyMate()
                st.rerun()
        
        # Information
        st.markdown("---")
        st.markdown("### üéØ How to Use")
        st.markdown("""
        1. Upload your PDF study materials
        2. Click 'Process Documents'
        3. Ask questions about your content
        4. Get AI-powered answers with context
        """)
        
        st.markdown("---")
        st.markdown("### üîß Technologies")
        st.markdown("""
        - **IBM Granite 3.3-2B** - Answer Generation
        - **SentenceTransformers** - Embeddings
        - **FAISS** - Semantic Search
        - **PyMuPDF** - PDF Processing
        """)
    
    # Main Content Area
    if not st.session_state.processed:
        st.info("üëà Please upload and process your PDF documents to get started!")
        
        # Example questions
        st.markdown("### üí° Example Questions You Can Ask:")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("""
            - What are the main concepts in Chapter 3?
            - Explain the theory of relativity
            - Summarize the key findings
            - What is the definition of photosynthesis?
            """)
        with col2:
            st.markdown("""
            - Compare X and Y concepts
            - What are the applications of this theory?
            - List the important dates mentioned
            - Explain this formula step by step
            """)
    else:
        # Question Input
        st.markdown("### üí¨ Ask Your Question")
        question = st.text_input(
            "Type your question here...",
            placeholder="e.g., What are the main concepts discussed in this chapter?",
            label_visibility="collapsed"
        )
        
        col1, col2 = st.columns([3, 1])
        with col1:
            ask_button = st.button("üîç Get Answer", use_container_width=True)
        with col2:
            clear_chat = st.button("üßπ Clear Chat", use_container_width=True)
        
        if clear_chat:
            st.session_state.chat_history = []
            st.rerun()
        
        # Process Question
        if ask_button and question:
            with st.spinner("Searching through your documents..."):
                # Retrieve relevant chunks
                relevant_chunks = studymate.retrieve_relevant_chunks(question, top_k=3)
                
                if not relevant_chunks:
                    st.error("No relevant information found. Try rephrasing your question.")
                else:
                    # Combine context
                    context = "\n\n".join([chunk for chunk, _ in relevant_chunks])
                    
                    # Generate answer
                    with st.spinner("Generating answer..."):
                        answer = studymate.generate_answer(question, context)
                    
                    # Add to chat history
                    st.session_state.chat_history.append({
                        "question": question,
                        "answer": answer,
                        "contexts": relevant_chunks
                    })
        
        # Display Chat History
        if st.session_state.chat_history:
            st.markdown("---")
            st.markdown("### üìñ Conversation History")
            
            for idx, chat in enumerate(reversed(st.session_state.chat_history)):
                with st.container():
                    st.markdown(f"**Q{len(st.session_state.chat_history) - idx}:** {chat['question']}")
                    
                    st.markdown(f'<div class="answer-box"><strong>Answer:</strong><br>{chat["answer"]}</div>', 
                              unsafe_allow_html=True)
                    
                    with st.expander("üìö View Source Context"):
                        for ctx_idx, (context, distance) in enumerate(chat['contexts'], 1):
                            st.markdown(f'<div class="context-box"><strong>Source {ctx_idx}</strong> (Relevance: {1/(1+distance):.2%})<br>{context[:300]}...</div>', 
                                      unsafe_allow_html=True)
                    
                    st.markdown("---")

if __name__ == "__main__":
    main()
</body>
</html>